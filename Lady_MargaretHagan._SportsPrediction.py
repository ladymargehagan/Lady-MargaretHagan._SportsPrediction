# -*- coding: utf-8 -*-
"""Assignment 2: Regression Lady Hagan

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BYwszji77FCdLkO8dfPSqXE0w82gPg5q
"""
 

import numpy as np
import pandas as pd


male_p = pd.read_csv('/Users/margehagan/Downloads/male_players (legacy).csv')

male_p.head()

"""Data Pre-Processing"""

male_p.info()

male_p.describe()

print(f'Shape of the data: {male_p.shape[1]}')

#handling missing data

na = []
no_na = []

for col in male_p.columns:
  if male_p[col].isnull().sum() / len(male_p) > 0.3:
    na.append(col)
  else:
    no_na.append(col)

na

m_players = male_p.drop(columns = na)

m_players.shape[1]

m_players.info()

#checking correltions

correlation_matrix = m_players.select_dtypes(include=[np.number]).corr()

corr = correlation_matrix['overall'].sort_values(ascending=False)

corr

#selected features based on correlation and inference about real-world player performance determinants

selected_features = ['overall', 'passing', 'dribbling', 'attacking_short_passing', 'shooting',
                  'skill_long_passing', 'skill_ball_control', 'attacking_crossing',
                  'skill_fk_accuracy', 'mentality_vision', 'mentality_aggression',
                  'mentality_positioning', 'mentality_penalties', 'movement_reactions',
                  'potential', 'wage_eur', 'value_eur', 'international_reputation', 'physic',
                  'age', 'goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking',
                  'goalkeeping_positioning', 'goalkeeping_reflexes'
]

new_player_features = m_players[selected_features]

new_player_features.columns

new_player_features.fillna(new_player_features.median(), inplace=True)

new_player_features

"""Feature Engineering"""

#merging similar features

combined_passing = ['passing','attacking_short_passing','skill_long_passing']
combined_dribbling = ['dribbling','skill_ball_control']
combined_mentality = ['mentality_vision','mentality_aggression','mentality_positioning','mentality_penalties']

new_player_features['combined_passing'] = new_player_features[combined_passing].mean(axis=1)
new_player_features['combined_dribbling'] = new_player_features[combined_dribbling].mean(axis=1)
new_player_features['combined_mentality'] = new_player_features[combined_mentality].mean(axis=1)

new_player_features.drop(columns=combined_passing, inplace=True)
new_player_features.drop(columns=combined_dribbling, inplace=True)
new_player_features.drop(columns=combined_mentality, inplace=True)

new_player_features

#converting all data types to make it easier for the model to train

new_player_features = new_player_features.astype(int)

new_player_features.info()

#packaging previous steps into a function

def preprocessing(df):
  # Getting general informaiton about the data
  df.info()
  df.describe()
  print(f'Shape of the data: {df.shape[1]}')

  # Handling missing data
  na = []
  no_na = []
  for col in df.columns:
    if df[col].isnull().sum() / len(df) > 0.3:
        na.append(col)
    else:
        no_na.append(col)

  #dropping na columns
  df = df.drop(columns=na)

  #Selecting optimal features based on correlation and inference

  correlation_matrix = df.select_dtypes(include=[np.number]).corr()
  corr = correlation_matrix['overall'].sort_values(ascending=False)
  selected_features = ['overall', 'passing', 'dribbling', 'potential', 'attacking_short_passing', 'shooting',
                  'skill_long_passing', 'skill_ball_control', 'attacking_crossing',
                  'skill_fk_accuracy', 'mentality_vision', 'mentality_aggression',
                  'mentality_positioning', 'mentality_penalties', 'movement_reactions',
                   'wage_eur', 'value_eur', 'international_reputation', 'physic',
                  'age', 'goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking',
                  'goalkeeping_positioning', 'goalkeeping_reflexes'
]

  new_features = df[selected_features]

  #Filling missing values with median

  new_features.fillna(new_features.median(), inplace=True)

  #merging similar attributes
  combined_passing = ['passing', 'attacking_short_passing', 'skill_long_passing']
  combined_dribbling = ['dribbling', 'skill_ball_control']
  combined_mentality = ['mentality_vision', 'mentality_aggression', 'mentality_positioning', 'mentality_penalties']

  new_features['combined_passing'] = new_features[combined_passing].mean(axis=1)
  new_features['combined_dribbling'] = new_features[combined_dribbling].mean(axis=1)
  new_features['combined_mentality'] = new_features[combined_mentality].mean(axis=1)

  new_features.drop(columns=combined_passing, inplace=True)
  new_features.drop(columns=combined_dribbling, inplace=True)
  new_features.drop(columns=combined_mentality, inplace=True)

  #converting to int to make it simpler for model to train
  new_features = new_features.astype(int)
  return new_features

"""Traning model"""

y = new_player_features['overall']
X = new_player_features.drop('overall', axis=1)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled = scaler.fit_transform(X)

X = pd.DataFrame(scaled, columns=X.columns)

X.head()

y.head()

from sklearn.model_selection import train_test_split

Xtrain,Xtest,Ytrain,Ytest = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(random_state=42)

rf.fit(Xtrain,Ytrain)

y_pred = rf.predict(Xtest)

y_pred

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_pred, Ytest)
rmse = np.sqrt(mean_squared_error(y_pred, Ytest))
r2 = r2_score(y_pred, Ytest)

print(f"Mean Absolute Error = {mae}")
print(f"Root Mean Squared Error = {rmse}")
print(f"R2 Score = {r2}")

"""XGB Regressor"""

from xgboost import XGBRegressor

xgb_regressor = XGBRegressor(random_state=42)

xgb_regressor.fit(Xtrain,Ytrain)

y_pred = xgb_regressor.predict(Xtest)

mae = mean_absolute_error(y_pred,Ytest)
rmse = np.sqrt(mean_squared_error(y_pred,Ytest))
r2 = r2_score(y_pred,Ytest)

print(f"Mean Absolute Error = {mae}")
print(f"Root Mean Squared Error = {rmse}")
print(f"R2 Score = {r2}")

"""Gradient Boosting Regressor"""

from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor(random_state=42)

gbr.fit(Xtrain,Ytrain)

y_pred = gbr.predict(Xtest)

mae = mean_absolute_error(y_pred, Ytest)
rmse = np.sqrt(mean_squared_error(y_pred, Ytest))
r2 = r2_score(y_pred, Ytest)

print(f"Mean Absolute Error = {mae}")
print(f"Root Mean Squared Error = {rmse}")
print(f"R2 Score = {r2}")

"""Defining functions for metrics"""

def train_rf(Xtrain, Xtest, Ytrain, Ytest):
    rf = RandomForestRegressor(random_state=42)
    rf.fit(Xtrain, Ytrain)
    y_pred_rf = rf.predict(Xtest)
    mae_rf = mean_absolute_error(Ytest, y_pred_rf)
    rmse_rf = np.sqrt(mean_squared_error(Ytest, y_pred_rf))
    r2_rf = r2_score(Ytest, y_pred_rf)
    print(f"Mean Absolute Error for Random Forest Model: {mae_rf}, RMSE: {rmse_rf}, R2: {r2_rf}")
    joblib.dump(rf, '/content/drive/MyDrive/Colab Notebooks/rfm_new.pkl')

def train_xgb(Xtrain, Xtest, Ytrain, Ytest):
    xgb = XGBRegressor(random_state=42)
    xgb.fit(Xtrain, Ytrain)
    y_pred_xgb = xgb.predict(Xtest)
    mae_xgb = mean_absolute_error(Ytest, y_pred_xgb)
    rmse_xgb = np.sqrt(mean_squared_error(Ytest, y_pred_xgb))
    r2_xgb = r2_score(Ytest, y_pred_xgb)
    print(f"Mean Absolute Error for XGB Model: {mae_xgb}, RMSE: {rmse_xgb}, R2: {r2_xgb}")
    joblib.dump(xgb, '/content/drive/MyDrive/Colab Notebooks/xgboost_model_new.pkl')

def train_gb(Xtrain, Xtest, Ytrain, Ytest):
    gbr = GradientBoostingRegressor(random_state=42)
    gbr.fit(Xtrain, Ytrain)
    y_pred_gbr = gbr.predict(Xtest)
    mae_gbr = mean_absolute_error(Ytest, y_pred_gbr)
    rmse_gbr = np.sqrt(mean_squared_error(Ytest, y_pred_gbr))
    r2_gbr = r2_score(Ytest, y_pred_gbr)
    print(f"Mean Absolute Error for Gradient Boosting Model: {mae_gbr}, RMSE: {rmse_gbr}, R2: {r2_gbr}")
    joblib.dump(gbr, '/content/drive/MyDrive/Colab Notebooks/gbn_new.pkl')

"""Cross Validation with Grid Search"""

from sklearn.model_selection import GridSearchCV

rf_params = {'n_estimators':[50, 100],'max_depth':[None, 10],'min_samples_split':[2, 5]}
xgb_params = {'n_estimators':[50, 100],'max_depth': [4, 5], 'learning_rate': [0.1, 0.01]}

rf_grid_search = GridSearchCV(estimator=rf, param_grid=rf_params, cv=3, n_jobs=-1, verbose=2)

rf_grid_search.fit(Xtrain, Ytrain)

xgb_grid_search = GridSearchCV(estimator= xgb_regressor, param_grid=xgb_params, cv=3, n_jobs=-1, verbose=2)

xgb_grid_search.fit(Xtrain, Ytrain)

best_rf_model = rf_grid_search.best_estimator_
print("Best Random Forest parameters:", rf_grid_search.best_params_)

best_xgb_model = xgb_grid_search.best_estimator_
print("Best XGB Regressor parameters:", xgb_grid_search.best_params_)

import joblib

joblib.dump(best_rf_model,'/content/drive/MyDrive/Colab Notebooks/best_rf_model.pkl')

joblib.dump(best_xgb_model, '/content/drive/MyDrive/Colab Notebooks/best_xgb_model.pkl')

#Make predictions with the best RF model

y_pred_best_rf = best_rf_model.predict(Xtest)
mae_best_rf = mean_absolute_error(Ytest,y_pred_best_rf)
print(f"Mean Absolute Error for best Random Forest model = {mae_best_rf}")

#Make predictions with the best XGB model

y_pred_best_xgb = best_xgb_model.predict(Xtest)
mae_best_xgb = mean_absolute_error(Ytest,y_pred_best_xgb)
print(f"Mean Absolute Error for Best XGB model= {mae_best_xgb}")

#consolidating this process into a function to make testing new data easier
def tuning(X_train, Y_train):
    rf_params = {'n_estimators': [50, 100], 'max_depth': [None, 10], 'min_samples_split': [2, 5]}
    xgb_params = {'n_estimators': [50, 100], 'max_depth': [4, 5], 'learning_rate': [0.1, 0.01]}

    rf_grid_search = GridSearchCV(estimator=RandomForestRegressor(),param_grid=rf_params,cv=3,n_jobs=-1,verbose=2)
    xgb_grid_search = GridSearchCV(estimator=XGBRegressor(),param_grid=xgb_params,cv=3, n_jobs=-1, verbose=2)

    rf_grid_search.fit(X_train, Y_train)
    xgb_grid_search.fit(X_train, Y_train)

    best_rf_model = rf_grid_search.best_estimator_
    best_xgb_model = xgb_grid_search.best_estimator_

    joblib.dump(best_rf_model,'/content/drive/MyDrive/Colab Notebooks/best_rf_model.pkl')
    joblib.dump(best_xgb_model,'/content/drive/MyDrive/Colab Notebooks/best_xgb_model.pkl')

    return best_rf_model, best_xgb_model

"""Ensemble model"""

from sklearn.ensemble import VotingRegressor

em = VotingRegressor(estimators=[('rf',rf),('xgb', xgb_regressor)])

em.fit(Xtrain, Ytrain)

em_pred = em.predict(Xtest)

#Evaluating model predictions
mae_ensemble = mean_absolute_error(em_pred, Ytest)
rmse_ensemble = np.sqrt(mean_squared_error(em_pred, Ytest))
print(f"Mean Absolute Error for Ensemble model = {mae_ensemble}")
print(f"Root Mean Squared Error for Ensemble model:{rmse_ensemble}")

joblib.dump(em, '/content/drive/MyDrive/Colab Notebooks/ensemble_model.pkl')

"""Training model with new dataset"""

players_22 = pd.read_csv('/content/drive/MyDrive/Players 22-1.csv')

new_df_players = preprocessing(players_22)

"""Training model with new data set"""

y_new = new_df_players['overall']
X_new = new_df_players.drop('overall', axis=1)
scaled_new = scaler.fit_transform(X_new)
X_new = pd.DataFrame(scaled_new, columns=X_new.columns)

X_new.head()

y_new.value_counts()

# Fixing a value error I encountered with classes 92 and 93
y_new = new_df_players['overall'].replace({93: 92})

y_new.value_counts()

X_train,X_test,Y_train,Y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=42, stratify=y_new)

train_rf(X_train,X_test,Y_train,Y_test)

train_xgb(X_train,X_test,Y_train,Y_test)

train_gb(X_train,X_test,Y_train,Y_test)

"""Cross Validation using grid search"""

tuning(X_train,Y_train)

em = VotingRegressor(estimators=[('rf',rf),('xgb', xgb_regressor)])

em.fit(X_train, Y_train)

em_pred = em.predict(X_test)

#Evaluating model predictions
mae_ensemble = mean_absolute_error(em_pred, Y_test)
rmse_ensemble = np.sqrt(mean_squared_error(em_pred, Y_test))

print(f"Mean Absolute Error for Ensemble model = {mae_ensemble}")
print(f"Root Mean Squared Error for Ensemble model:{rmse_ensemble}")

joblib.dump(em, '/Users/margehagan/Downloads/em.pkl')

"""Creating a pickle to save data"""

import pickle

pickle.dump(scaled_new, open('/Users/margehagan/Downloads/scaler_ensemble.pkl', 'wb'))

